---
title: "Class 7: Hands on with Principal Component Analysis (PCA)"
author: "Liana Melikian (A16675734)"
format: pdf
---
#Clustering

We will start today's lab with clustering methods, in particular so-called K-means. The main function for this in R is `kmeans()`

Let's try it on some made up data where we know what the answer should be.

```{r}
x=rnorm(10000,mean=3)
hist(x)
```

60 points
```{r}
tmp=c(rnorm(30,mean=3), rnorm(30,-3))
x=cbind(x=tmp,y=rev(tmp))
head(x)
```

We can pass this to the R `plot()` function for a quick.
```{r}
plot(x)
```

```{r}
k=kmeans(x,centers=2,nstart=20)
k
```

Q1. How many points are in each cluster?

```{r}
k$size
```

Q2. Cluster membership?

```{r}
k$cluster
```

Q3. Cluster centers?

```{r}
k$centers
```

Q4. PLot my clustering results
```{r}
plot(x,col=k$cluster,pch=16)
```

>Q5. Cluster the data again with kmeans() into 4 groups and plot the results.

```{r}
k4=kmeans(x,centers=4, nstart=20)
plot(x,col=k4$cluster, pch=16)
```

K-means is very popular mostly because it is fast and relatively straightforward to run and understand. It has a big limitation in that you need to tell it how many groups (k, or centers) you want.

#Hierarchical clustering

The main function in base R is called `hclust()`. You have to pass it in a "distance matrix" not just your input data.

You can generate a distance matrix with the `dist()` function.

```{r}
hc=hclust(dist(x))
hc
```

```{r}
plot(hc)
```

To find the clusters (cluster membership vector) from a `hclust()` result we can "cut" the tree at a certain height that we like. For this we can use the `cutree()` function.

```{r}
plot(hc)
abline(h=8,col="red")
grps =cutree(hc,h=8)
```

```{r}
table(grps)
```

>Q6. Plot our hclust results.

```{r}
plot(x,col=grps)
```

#PCA Component Analysis

##PCA of UK food data

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

Q1.
```{r}
dim(x)
```

```{r}
head(x)
```

# Note how the minus indexing works
```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```


```{r}
x <- read.csv(url, row.names=1)
head(x)
dim(x)
```

Q2. I prefer the second approach because if the first approach is run more than once, it keeps removing a column with every run until there is an error.

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

Q3. 
```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

A pairs plot can be useful if we don't have too many dimensions.

Q5.
```{r}
pairs(x, col=rainbow(17), pch=16,cex=2)
```

Q6. There is greater spread between N. Ireland and other countries. It is an outlier compared with the other countries when comparing different foods.

##Principal Component Analysis (PCA)

PCA can help us make sense of these types of datasets. Let's see how it works. 

The main function in "base" R is called `prcomp()`. In this case we want to first take the transpose of our input `x` so the columns are the food types and the countries are the rows.

```{r}
head(t(x))
```

```{r}
pca=prcomp(t(x))
summary(pca)
```

Q7.
```{r}
pca$x
plot(pca$x[,1],pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```
Q8.
```{r}
plot(pca$x[,1],pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2],  colnames(x),col=c("orange","red","blue","darkgreen"),pch=16)
```

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
## or the second row here...
z <- summary(pca)
z$importance
```
```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```


The "loadings" tell us how much the original variables (in our case the foods) contribute to the new variables i.e. the PCs.

```{r}
head(pca$rotation)
```

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```
















