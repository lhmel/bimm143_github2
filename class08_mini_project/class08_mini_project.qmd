---
title: "Class 8 Mini-Project: Unsupervised Learning Analysis of Human Breast Cancer Cells"
author: "Liana Melikian (A16675734)"
format: pdf
---

```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)
```

```{r}
wisc.data <- wisc.df[,-1]
diagnosis=as.factor(wisc.df$diagnosis)
```

Q1. How many observations are in this dataset? 569
```{r}
nrow(wisc.data)
```

Q2. How many of the observations have a malignant diagnosis? 212
```{r}
table(wisc.df$diagnosis)
sum(wisc.df$diagnosis=="M")
```
Q3. How many variables/features in the data are suffixed with _mean? 10
```{r}
x=colnames(wisc.df)
length(grep("_mean",x))
```

```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```
Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
44.27%

Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
The first 3 PCs are required


Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
The first 7 PCs are required

```{r}
biplot(wisc.pr)
```

Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
This plot is very complicated and cluttered. It is almost impossible to read and nearly nothing can be understood from it.

```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,2],col=diagnosis)
```

Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots? The malignant and benign patients are cluttered separate from each other.
```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,3],col=diagnosis)
```

```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
library(ggplot2)
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis)+
  geom_point()
```

```{r}
pr.var=wisc.pr$sdev^2
head(pr.var)
```

```{r}
pve=pr.var/sum(pr.var)
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```
Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? 
```{r}
wisc.pr$rotation[,1]["concave.points_mean"]
```

Q10. What is the minimum number of principal components required to explain 80% of the variance of the data? 5 principal components 


```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist, method="complete")
```

Q11.Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters? At around height 19
```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```
```{r}
wisc.hclust.clusters = cutree(wisc.hclust,k=4)
table(wisc.hclust.clusters, diagnosis)
```
Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10? No, k=4 has the best cluster vs diagnoses match as it maximizes the separation betwen malignant and benign cells the best.
```{r}
wisc.hclust.clusters = cutree(wisc.hclust,k=10)
table(wisc.hclust.clusters, diagnosis)
```
Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
Ward.d2 gives my favorite results for the dataset because there is a clear distinction between benign and malignant cells, and it is overall much easier to analyze.

```{r}
hc.single=hclust(data.dist, method = "single")
plot(hc.single)
```

```{r}
hc.complete=hclust(data.dist, method = "complete")
plot(hc.complete)
```

```{r}
hc.average=hclust(data.dist, method = "average")
plot(hc.average)
```

```{r}
hc.ward2=hclust(data.dist, method = "ward.D2")
plot(hc.ward2)
```


##Combining Methods

This approach will take not original data but our PCA results and works with them.

```{r}
d=dist(wisc.pr$x[,1:3])
wisc.pr.hclust=hclust(d,method="ward.D2")
plot(wisc.pr.hclust)
```
Generate 2 cluster groups from this hclust object.

```{r}
grps=cutree(wisc.pr.hclust,k=2)
grps
```

```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,2],col=grps)
```

```{r}
table(grps)
```

```{r}
table(diagnosis)
```

```{r}
table(diagnosis, grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```
```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
```

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=4)
table(wisc.pr.hclust.clusters)
```

Q15. How well does the newly created model with four clusters separate out the two diagnoses?
It does a great job, the separation between malignant and benign cell is well optimized.





